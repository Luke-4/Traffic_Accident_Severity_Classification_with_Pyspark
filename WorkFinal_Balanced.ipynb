{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5a2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000196936A2DF0>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, MultilayerPerceptronClassifier,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark=spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"US_accidents\") \\\n",
    "    .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa8da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev1 = spark.read.options(header=\"True\", InferSchema=\"True\", nullValue=\"null\" ).csv(\"dataset/severity/Severity=1/part-00000-79480ebc-ce62-4cda-b67d-03599fb35b85.c000.csv\")\n",
    "sev2 = spark.read.options(header=\"True\", InferSchema=\"True\", nullValue=\"null\" ).csv(\"dataset/severity/Severity=2/part-00000-79480ebc-ce62-4cda-b67d-03599fb35b85.c000.csv\")\n",
    "sev3 = spark.read.options(header=\"True\", InferSchema=\"True\", nullValue=\"null\" ).csv(\"dataset/severity/Severity=3/part-00000-79480ebc-ce62-4cda-b67d-03599fb35b85.c000.csv\")\n",
    "sev4 = spark.read.options(header=\"True\", InferSchema=\"True\", nullValue=\"null\" ).csv(\"dataset/severity/Severity=4/part-00000-79480ebc-ce62-4cda-b67d-03599fb35b85.c000.csv\")\n",
    "sev1=sev1.withColumn(\"Severity\", lit(1)).limit(2000)#max 25499\n",
    "sev2=sev2.withColumn(\"Severity\", lit(2)).limit(2000)\n",
    "sev3=sev3.withColumn(\"Severity\", lit(3)).limit(2000)\n",
    "sev4=sev4.withColumn(\"Severity\", lit(4)).limit(2000)\n",
    "df=sev1.union(sev2).union(sev3).union(sev4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e599d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Severity|count|\n",
      "+--------+-----+\n",
      "|       1| 2000|\n",
      "|       2| 2000|\n",
      "|       3| 2000|\n",
      "|       4| 2000|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf=df.groupby(\"Severity\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44fb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot one encoder\n",
    "inputIndexer=[\"Street\",\"Side\",\"City\",\"County\",\"State\",\"Zipcode\",\"Wind_Direction\",\n",
    "              \"Weather_Condition\"]\n",
    "\n",
    "outputIndexer=[\"Streetindex\",\"Sideindex\",\"Cityindex\",\"Countyindex\",\"Stateindex\",\"Zipcodeindex\",\n",
    "               \"Wind_Directionindex\",\"Weather_Conditionindex\"]\n",
    "\n",
    "outputEncoder=[\"Streetfinal\",\"Sidefinal\",\"Cityfinal\",\"Countyfinal\",\"Statefinal\",\"Zipcodefinal\",\n",
    "              \"Wind_Directionfinal\",\"Weather_Conditionfinal\"]\n",
    "\n",
    "numericCols = [\"Start_Lat\", \"Start_Lng\", \"Distance(mi)\", \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "               \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Amenity\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\"Roundabout\",\n",
    "               \"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"dayofweek\",\"year\",\"month\",\"dayofmonth\",\"hour\",\"Duration\"]\n",
    "\n",
    "#create a string indexer\n",
    "indexer = StringIndexer(inputCols=inputIndexer, outputCols=outputIndexer, handleInvalid=\"error\")\n",
    "\n",
    "#create onehotencoder\n",
    "#encoder = OneHotEncoder(inputCols=outputIndexer, outputCols=outputEncoder)\n",
    "\n",
    "#make a vectorassembler(for spark all the features must be in one column this joins all the columns in one)\n",
    "vector_assembler=VectorAssembler(inputCols=numericCols+outputIndexer, outputCol=\"features\")\n",
    "\n",
    "#featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a61abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the pipeline\n",
    "from pyspark.ml import Pipeline \n",
    "pipe= Pipeline(stages=[indexer, vector_assembler])\n",
    "#, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6062d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transform the data\n",
    "newdf = pipe.fit(df).transform(df)\n",
    "newdf=newdf.select([\"features\", \"Severity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5e01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and test sets\n",
    "training, test = newdf.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463dbb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1971507171927432"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the correlation between two features\n",
    "df.stat.corr('Severity','Distance(mi)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc7de3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|Severity|\n",
      "+--------------------+--------+\n",
      "|(34,[0,1,2,3,4,5,...|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b02d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression:::\n",
    "# Fit the model on the training data\n",
    "logistic = LogisticRegression(labelCol=\"Severity\", featuresCol=\"features\", maxIter=10, regParam=0, elasticNetParam=0.8).fit(training)\n",
    "# Create predictions for the testing data \n",
    "predictionlogistic = logistic.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b40d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"Severity\", featuresCol=\"features\", maxBins=10257).fit(training)\n",
    "\n",
    "# Make predictions.\n",
    "predictionsdt = dt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfcae783",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\", numTrees=10,maxDepth=8, maxBins=10257).fit(training)\n",
    "\n",
    "# Make predictions.\n",
    "predictionsrf = rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a15362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Random Forest Classifier Evaluation ###########\n",
      "\n",
      "True positive rate for severity:1 = 0.9275590551181102\n",
      "True positive rate for severity:2 = 0.7445742904841403\n",
      "True positive rate for severity:3 = 0.6830065359477124\n",
      "True positive rate for severity:4 = 0.5054545454545455\n",
      "\n",
      "False positive rate for severity:1 = 0.0704145371947757\n",
      "False positive rate for severity:2 = 0.06622148024485253\n",
      "False positive rate for severity:3 = 0.12275784753363228\n",
      "False positive rate for severity:4 = 0.10996749729144095\n",
      "\n",
      "fMeasure for severity:1 = 0.8738872403560831\n",
      "fMeasure for severity:2 = 0.7663230240549829\n",
      "fMeasure for severity:3 = 0.6693354683746998\n",
      "fMeasure for severity:4 = 0.5392822502424831\n",
      "\n",
      "f1 Score: 0.7179400806818571\n",
      "False Positive RateR: 0.09181545241495188\n",
      "True Positive Rate: 0.7224540901502504\n",
      "Precision: 0.716560329622679\n",
      "Hamming Loss:0.2775459098497496\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator#evaluate the models\n",
    "#Evaluating the model\n",
    "#forest:::\n",
    "\n",
    "predictionsrf = predictionsrf.select(\"prediction\", \"Severity\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol= \"Severity\")\n",
    "\n",
    "TPRseverity1=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "TPRseverity2=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "TPRseverity3=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "TPRseverity4=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "FPRseverity1=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "FPRseverity2=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "FPRseverity3=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "FPRseverity4=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "fMeasureseverity1=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1})\n",
    "fMeasureseverity2=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 2})\n",
    "fMeasureseverity3=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 3})\n",
    "fMeasureseverity4=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "accuracy=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"accuracy\"})\n",
    "f1=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedRecall\"})\n",
    "weightedTruePositiveRate=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "weightedFalsePositiveRate=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "weightedFMeasure=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedFMeasure\"})#\n",
    "hammingLoss=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"hammingLoss\"})\n",
    "print(\"########### Random Forest Classifier Evaluation ###########\\n\")\n",
    "print(\"True positive rate for severity:1 = \" + str(TPRseverity1))\n",
    "print(\"True positive rate for severity:2 = \" + str(TPRseverity2))\n",
    "print(\"True positive rate for severity:3 = \" + str(TPRseverity3))\n",
    "print(\"True positive rate for severity:4 = \" + str(TPRseverity4))\n",
    "\n",
    "print(\"\\nFalse positive rate for severity:1 = \" + str(FPRseverity1))\n",
    "print(\"False positive rate for severity:2 = \" + str(FPRseverity2))\n",
    "print(\"False positive rate for severity:3 = \" + str(FPRseverity3))\n",
    "print(\"False positive rate for severity:4 = \" + str(FPRseverity4))\n",
    "\n",
    "print(\"\\nfMeasure for severity:1 = \" + str(fMeasureseverity1))\n",
    "print(\"fMeasure for severity:2 = \" + str(fMeasureseverity2))\n",
    "print(\"fMeasure for severity:3 = \" + str(fMeasureseverity3))\n",
    "print(\"fMeasure for severity:4 = \" + str(fMeasureseverity4))\n",
    "\n",
    "\n",
    "print(\"\\nf1 Score: %s\\nFalse Positive Rate %s\\nTrue Positive Rate: %s\\nPrecision: %s\\nHamming Loss:%s\"\n",
    "      % (f1, weightedFalsePositiveRate, weightedTruePositiveRate, weightedPrecision, hammingLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02eee3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Logistoc Regression Evaluation ###########\n",
      "\n",
      "True positive rate for severity:1 = 0.7716535433070866\n",
      "True positive rate for severity:2 = 0.6393989983305509\n",
      "True positive rate for severity:3 = 0.5980392156862745\n",
      "True positive rate for severity:4 = 0.4509090909090909\n",
      "\n",
      "False positive rate for severity:1 = 0.14026121521862578\n",
      "False positive rate for severity:2 = 0.1196438508625487\n",
      "False positive rate for severity:3 = 0.11434977578475336\n",
      "False positive rate for severity:4 = 0.13163596966413868\n",
      "\n",
      "fMeasure for severity:1 = 0.7142857142857143\n",
      "fMeasure for severity:2 = 0.6399331662489558\n",
      "fMeasure for severity:3 = 0.6192893401015229\n",
      "fMeasure for severity:4 = 0.47646493756003844\n",
      "\n",
      "f1 Score: 0.6168414803650694\n",
      "False Positive RateR: 0.12650850769033364\n",
      "True Positive Rate: 0.6206176961602672\n",
      "Precision: 0.616274552795516\n",
      "Hamming Loss:0.37938230383973287\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator#evaluate the models\n",
    "#Evaluating the model\n",
    "#Logistic Regression:::\n",
    "\n",
    "#predictionlogistic.groupBy(\"Severity\", \"prediction\").count().show()\n",
    "predictionAndLabelslr = predictionlogistic.select(\"prediction\", \"Severity\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol= \"Severity\")\n",
    "\n",
    "TPRseverity1=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "TPRseverity2=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "TPRseverity3=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "TPRseverity4=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "FPRseverity1=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "FPRseverity2=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "FPRseverity3=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "FPRseverity4=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "fMeasureseverity1=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1})\n",
    "fMeasureseverity2=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 2})\n",
    "fMeasureseverity3=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 3})\n",
    "fMeasureseverity4=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "accuracy=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"accuracy\"})\n",
    "f1=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedRecall\"})\n",
    "weightedTruePositiveRate=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "weightedFalsePositiveRate=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "weightedFMeasure=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedFMeasure\"})#\n",
    "hammingLoss=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"hammingLoss\"})\n",
    "print(\"########### Logistoc Regression Evaluation ###########\\n\")\n",
    "\n",
    "print(\"True positive rate for severity:1 = \" + str(TPRseverity1))\n",
    "print(\"True positive rate for severity:2 = \" + str(TPRseverity2))\n",
    "print(\"True positive rate for severity:3 = \" + str(TPRseverity3))\n",
    "print(\"True positive rate for severity:4 = \" + str(TPRseverity4))\n",
    "\n",
    "print(\"\\nFalse positive rate for severity:1 = \" + str(FPRseverity1))\n",
    "print(\"False positive rate for severity:2 = \" + str(FPRseverity2))\n",
    "print(\"False positive rate for severity:3 = \" + str(FPRseverity3))\n",
    "print(\"False positive rate for severity:4 = \" + str(FPRseverity4))\n",
    "\n",
    "print(\"\\nfMeasure for severity:1 = \" + str(fMeasureseverity1))\n",
    "print(\"fMeasure for severity:2 = \" + str(fMeasureseverity2))\n",
    "print(\"fMeasure for severity:3 = \" + str(fMeasureseverity3))\n",
    "print(\"fMeasure for severity:4 = \" + str(fMeasureseverity4))\n",
    "\n",
    "print(\"\\nf1 Score: %s\\nFalse Positive Rate %s\\nTrue Positive Rate: %s\\nPrecision: %s\\nHamming Loss:%s\"\n",
    "      % (f1, weightedFalsePositiveRate, weightedTruePositiveRate, weightedPrecision, hammingLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e1e59ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Decision Tree Classifier Evaluation ###########\n",
      "\n",
      "True positive rate for severity:1 = 0.8125984251968504\n",
      "True positive rate for severity:2 = 0.7562604340567612\n",
      "True positive rate for severity:3 = 0.5310457516339869\n",
      "True positive rate for severity:4 = 0.5818181818181818\n",
      "\n",
      "False positive rate for severity:1 = 0.03691084611016468\n",
      "False positive rate for severity:2 = 0.08569838619922092\n",
      "False positive rate for severity:3 = 0.11378923766816143\n",
      "False positive rate for severity:4 = 0.19501625135427952\n",
      "\n",
      "fMeasure for severity:1 = 0.8486842105263158\n",
      "fMeasure for severity:2 = 0.7512437810945274\n",
      "fMeasure for severity:3 = 0.5701754385964911\n",
      "fMeasure for severity:4 = 0.5203252032520326\n",
      "\n",
      "f1 Score: 0.67781123905238\n",
      "False Positive Rate 0.10503742583933907\n",
      "True Positive Rate: 0.6736227045075125\n",
      "Precision: 0.6871938317249804\n",
      "Hamming Loss:0.32637729549248745\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator#evaluate the models\n",
    "#Evaluating the model\n",
    "#Tree:::\n",
    "\n",
    "#predictionlogistic.groupBy(\"Severity\", \"prediction\").count().show()\n",
    "predictionsdt = predictionsdt.select(\"prediction\", \"Severity\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol= \"Severity\")\n",
    "\n",
    "TPRseverity1=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "TPRseverity2=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "TPRseverity3=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "TPRseverity4=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"truePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "FPRseverity1=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 1})\n",
    "FPRseverity2=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 2})\n",
    "FPRseverity3=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 3})\n",
    "FPRseverity4=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"falsePositiveRateByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "fMeasureseverity1=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1})\n",
    "fMeasureseverity2=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 2})\n",
    "fMeasureseverity3=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 3})\n",
    "fMeasureseverity4=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 4})\n",
    "\n",
    "\n",
    "accuracy=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"accuracy\"})\n",
    "f1=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedRecall\"})\n",
    "weightedTruePositiveRate=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "weightedFalsePositiveRate=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "weightedFMeasure=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedFMeasure\"})#\n",
    "hammingLoss=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"hammingLoss\"})\n",
    "print(\"########### Decision Tree Classifier Evaluation ###########\\n\")\n",
    "\n",
    "print(\"True positive rate for severity:1 = \" + str(TPRseverity1))\n",
    "print(\"True positive rate for severity:2 = \" + str(TPRseverity2))\n",
    "print(\"True positive rate for severity:3 = \" + str(TPRseverity3))\n",
    "print(\"True positive rate for severity:4 = \" + str(TPRseverity4))\n",
    "\n",
    "print(\"\\nFalse positive rate for severity:1 = \" + str(FPRseverity1))\n",
    "print(\"False positive rate for severity:2 = \" + str(FPRseverity2))\n",
    "print(\"False positive rate for severity:3 = \" + str(FPRseverity3))\n",
    "print(\"False positive rate for severity:4 = \" + str(FPRseverity4))\n",
    "\n",
    "print(\"\\nfMeasure for severity:1 = \" + str(fMeasureseverity1))\n",
    "print(\"fMeasure for severity:2 = \" + str(fMeasureseverity2))\n",
    "print(\"fMeasure for severity:3 = \" + str(fMeasureseverity3))\n",
    "print(\"fMeasure for severity:4 = \" + str(fMeasureseverity4))\n",
    "\n",
    "print(\"\\nf1 Score: %s\\nFalse Positive Rate %s\\nTrue Positive Rate: %s\\nPrecision: %s\\nHamming Loss:%s\"\n",
    "      % (f1, weightedFalsePositiveRate, weightedTruePositiveRate, weightedPrecision, hammingLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2309101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1 Score logistic regression: 0.6168414803650694\n",
      "f1 Score Decision Tree: 0.67781123905238\n",
      "f1 Score Random Forest: 0.7179400806818571\n"
     ]
    }
   ],
   "source": [
    "f1lr=str(evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"f1\"}))\n",
    "f1dt=str(evaluator.evaluate(predictionsdt, {evaluator.metricName: \"f1\"}))\n",
    "f1rf=str(evaluator.evaluate(predictionsrf, {evaluator.metricName: \"f1\"}))\n",
    "print(\"\\nf1 Score logistic regression: %s\\nf1 Score Decision Tree: %s\\nf1 Score Random Forest: %s\"\n",
    "      % (f1lr, f1dt, f1rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b788ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Positive Rate Logistic Regression: 0.6206176961602672\n",
      "False Positive Rate Logistic Regression: 0.12650850769033364\n",
      "True Positive Rate Decision Tree: 0.6736227045075125\n",
      "False Positive Rate Decision Tree: 0.10503742583933907\n",
      "True Positive Rate Random Forest: 0.7224540901502504\n",
      "False Positive Rate Random Forest: 0.09181545241495188\n"
     ]
    }
   ],
   "source": [
    "tprlr=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "fprlr=evaluator.evaluate(predictionAndLabelslr, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "\n",
    "tprdt=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "fprdt=evaluator.evaluate(predictionsdt, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "\n",
    "tprrf=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedTruePositiveRate\"})\n",
    "fprrf=evaluator.evaluate(predictionsrf, {evaluator.metricName: \"weightedFalsePositiveRate\"})\n",
    "\n",
    "print(\"\\nTrue Positive Rate Logistic Regression: %s\\nFalse Positive Rate Logistic Regression: %s\\nTrue Positive Rate Decision Tree: %s\\nFalse Positive Rate Decision Tree: %s\\nTrue Positive Rate Random Forest: %s\\nFalse Positive Rate Random Forest: %s\"\n",
    "      % (tprlr, fprlr, tprdt, fprdt, tprrf, fprrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5147d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "#make the grid\n",
    "\n",
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, labelCol=\"Severity\", featuresCol=\"features\")\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, [0.1, 0.01])\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f09c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "#make the validator\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65983edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7df59a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_cd86e929c60f, numClasses=5, numFeatures=34\n"
     ]
    }
   ],
   "source": [
    "#get the hyperparams from cross validation and\n",
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40320852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6168414803650694\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "effb8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "#make the grid\n",
    "\n",
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "dttuned = DecisionTreeClassifier(labelCol=\"Severity\", featuresCol=\"features\", maxBins=10257)\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(dttuned.maxDepth, [5, 10, 15, 20])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46ff6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "#make the validator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol= \"Severity\")\n",
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=dttuned,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beae12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "# Use the model to predict the test set\n",
    "test_results = model.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results, {evaluator.metricName: \"f1\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
